job {
    env = "dev"
    enable.distributed.checkpointing = false
    statebackend {
      base.url = "s3://<flink-bucket-name>"
    }
  }
  kafka {
    broker-servers = "kafka-headless.kafka.svc.cluster.local:9092"
    producer.broker-servers = "kafka-headless.kafka.svc.cluster.local:9092"
    consumer.broker-servers = "kafka-headless.kafka.svc.cluster.local:9092"
    zookeeper = "kafka-headless.kafka.svc.cluster.local:2181"
    producer {
      max-request-size = 1572864
      batch.size = 98304
      linger.ms = 10
      compression = "snappy"
    }
    output.system.event.topic = ${job.env}".system.events"
    output.failed.topic = ${job.env}".failed"
  }
  task {
    parallelism = 1
    consumer.parallelism = 1
    checkpointing.interval = 30000
    checkpointing.pause.between.seconds = 5000
    restart-strategy.attempts = 3
    restart-strategy.delay = 30000 # in milli-seconds
  }

  redis.connection.timeout = 100
  redis {
    host = obsrv-redis-master.redis.svc.cluster.local
    port = 6379
  }

  redis-meta {
    host = obsrv-redis-master.redis.svc.cluster.local
    port = 6379
  }

  postgres {
    host = postgresql-hl.postgresql.svc.cluster.local
    port = 5432
    maxConnections = 2
    user = "postgres"
    password = "postgres"
    database = "obsrv"
  }

  lms-cassandra {
    host = "localhost"
    port = "9042"
  }


kafka {
  input.topic = ${job.env}".kafka.connector.in"
  output.topic = ${job.env}".kafka.connector.out"
  output.failed.topic = ${job.env}".failed"
  event.max.size = "1048576" # Max is only 1MB
  groupId = ${job.env}"-kafkaconnector-group"
  producer {
    max-request-size = 5242880
  }
}

task {
  consumer.parallelism = 1
  downstream.operators.parallelism = 1
}

connector.version = "1.0.0"