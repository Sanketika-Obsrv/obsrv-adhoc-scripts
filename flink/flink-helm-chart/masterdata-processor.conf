job {
    env = "dev"
    enable.distributed.checkpointing = false
    statebackend {
      base.url = "s3://<flink-bucket-name>"
    }
}
  kafka {
    broker-servers = "kafka-headless.kafka.svc.cluster.local:9092"
    producer.broker-servers = "kafka-headless.kafka.svc.cluster.local:9092"
    consumer.broker-servers = "kafka-headless.kafka.svc.cluster.local:9092"
    zookeeper = "kafka-headless.kafka.svc.cluster.local:2181"
    producer {
      max-request-size = 1572864
      batch.size = 98304
      linger.ms = 10
      compression = "snappy"
    }
    output.system.event.topic = ${job.env}".system.events"
    output.failed.topic = ${job.env}".failed"
  }
  task {
    parallelism = 1
    consumer.parallelism = 1
    checkpointing.interval = 30000
    checkpointing.pause.between.seconds = 5000
    restart-strategy.attempts = 3
    restart-strategy.delay = 30000 # in milli-seconds
  }

  redis.connection.timeout = 100
  redis {
    host = obsrv-redis-master.redis.svc.cluster.local
    port = 6379
  }

  redis-meta {
    host = obsrv-redis-master.redis.svc.cluster.local
    port = 6379
  }

  postgres {
    host = postgresql-hl.postgresql.svc.cluster.local
    port = 5432
    maxConnections = 2
    user = "postgres"
    password = "postgres"
    database = "obsrv"
  }

  lms-cassandra {
    host = "localhost"
    port = "9042"
  }
  
kafka {
  input.topic = ${job.env}".masterdata.ingest"
  output.raw.topic = ${job.env}".masterdata.raw"
  output.extractor.duplicate.topic = ${job.env}".masterdata.failed"
  output.failed.topic = ${job.env}".masterdata.failed"
  output.batch.failed.topic = ${job.env}".masterdata.extractor.failed"
  event.max.size = "1048576" # Max is only 1MB
  output.invalid.topic = ${job.env}".masterdata.failed"
  output.unique.topic = ${job.env}".masterdata.unique"
  output.duplicate.topic = ${job.env}".masterdata.failed"
  output.denorm.topic = ${job.env}".masterdata.denorm"
  output.transform.topic = ${job.env}".masterdata.transform"
  stats.topic = ${job.env}".masterdata.stats"
  groupId = ${job.env}"-masterdata-pipeline-group"

  producer {
    max-request-size = 5242880
  }
}

task {
  window.time.in.seconds = 5
  window.count = 30
  window.shards = 1400
  consumer.parallelism = 1
  downstream.operators.parallelism = 1
}

redis {
  database {
    extractor.duplication.store.id = 1
    preprocessor.duplication.store.id = 2
    key.expiry.seconds = 3600
  }
}

dataset.type = "master-dataset"% 
